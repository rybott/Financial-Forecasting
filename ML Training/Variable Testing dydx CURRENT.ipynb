{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping Through different variables to find the best combination\n",
    "\n",
    "## Results\n",
    "### Starting Data - 1 month\n",
    "- Best Results\n",
    "    1. Data Order =\n",
    "    2. Underlying Data Type =\n",
    "    3. Number of Feilds =\n",
    "    4. Derivatives =\n",
    "    5. TA's =\n",
    "- Best Combination for XGBoost\n",
    "    1. Data Order =\n",
    "    2. Underlying Data Type =\n",
    "    3. Number of Feilds =\n",
    "    4. Derivatives =\n",
    "    5. TA's =\n",
    "- Best Combination for Logistic Regression\n",
    "    1. Data Order =\n",
    "    2. Underlying Data Type =\n",
    "    3. Number of Feilds =\n",
    "    4. Derivatives =\n",
    "    5. TA's = \n",
    "- Best Combination for LSTM\n",
    "    1. Data Order =\n",
    "    2. Underlying Data Type =\n",
    "    3. Number of Feilds =\n",
    "    4. Derivatives =\n",
    "    5. TA's =\n",
    "\n",
    "## Data\n",
    "1. Data Order - Either the data is in the original order or the data is randomized after being turned into packets. This is because I believe randomizing the data will prevent overfitting better than it being in a predicable order, because I do not believe the packets should be linked, for best results they should be seen as independent by the data.\n",
    "2. Underlying Data Type - Either the data is Raw or a percent change. I am not sure whether using the raw data or a decimal change of the number is better for the derivatives.\n",
    "3. Number of fields - This is the number of look back periods in each packet\n",
    "4. Derivatives - Using the first derivative for ROC and the second derivative for acceleration, on both the raw data as well as different TA's\n",
    "5. TA's - Trend analysis that I want to use, for now we are using different EMA's and SMA's plus possibly RSI14 and Bolinger Bands, to determine if any of them will enhance the outcome\n",
    "\n",
    "## Process\n",
    "### Get the Data\n",
    "1. Start with 1 month of data for speed\n",
    "2. Continue with 1 year of data\n",
    "3. Based on the random data vs ordered data, take 1 year of random data\n",
    "    (Look at the the whole database and pull out a number of random entries equivalent to 1 years worth)\n",
    "4. Take the most efficient process and apply it to the whole dataset in subgroups, randomly assigning groups. Steps 3 and 4 are to prevent overfitting over a single Ticker\n",
    "\n",
    "### Processing \n",
    "1. I have 7 variables to test\n",
    "    1. Number of Fields - Number of fields in the packet (lookback period)\n",
    "    2. Derivative - Using the First and Second Derivative, and using it on:\n",
    "        - Raw Data\n",
    "        - Percent Change\n",
    "        - EMA8, SMA8, EMA20, SMA20\n",
    "        - Mix of these\n",
    "    3. EMAs\n",
    "    4. SMAs\n",
    "    5. Other\n",
    "    6. Underlying Data\n",
    "    7. Data Order\n",
    "2. Loop Through all of the variables and test the XGBoost on all of them\n",
    "3. Loop Through all of them and test Logistic Regression\n",
    "4. Loop Through all of them and test LSTM Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Datetime        open        high         low       close  volume  \\\n",
      "0 2023-01-03 04:00:00  129.391006  130.113998  129.386002  130.110001  5381.0   \n",
      "1 2023-01-03 04:01:00  129.977005  130.283005  129.972000  130.209000  6072.0   \n",
      "2 2023-01-03 04:02:00  130.285004  130.352005  130.270004  130.279007  1053.0   \n",
      "3 2023-01-03 04:03:00  130.294998  130.401993  130.289993  130.388000  1754.0   \n",
      "4 2023-01-03 04:04:00  130.384003  130.570999  130.378998  130.567001  3766.0   \n",
      "\n",
      "  Stock  interval  \n",
      "0  AAPL       1.0  \n",
      "1  AAPL       1.0  \n",
      "2  AAPL       1.0  \n",
      "3  AAPL       1.0  \n",
      "4  AAPL       1.0  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import os\n",
    "import csv\n",
    "\n",
    "con = duckdb.connect(r\"C:\\Users\\rybot\\OneDrive\\Databases\\datadump.duckdb\")\n",
    "\n",
    "qry = '''\n",
    "    SELECT * FROM Stocks\n",
    "    WHERE EXTRACT(YEAR FROM Datetime) = 2023 \n",
    "    AND Stock = 'AAPL' \n",
    "    AND Interval = 1\n",
    "'''\n",
    "\n",
    "df = con.execute(qry).fetchsub_df()\n",
    "# Sort the Data from oldest to newest\n",
    "df = df.sort_values(by='Datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trail_stoploss(row, tradingdata, whole_percentage):\n",
    "    idx = row.name\n",
    "    td = tradingdata\n",
    "    pct = whole_percentage/100\n",
    "    max_holdtime = 30 # Only hold the trade for twenty minutes\n",
    "\n",
    "    after_td = td.loc[idx:idx+max_holdtime+1] \n",
    "    \n",
    "    basis = td['close'].loc[idx]\n",
    "    max_price = basis   # Initially the max price is the basis - No Shorting to Start\n",
    "    min_price = basis - (basis*pct)\n",
    "    time_stop = idx + max_holdtime \n",
    "    time_counter = idx\n",
    "    \n",
    "    #print(f\"Basis: {basis}\")\n",
    "\n",
    "    for i, row in after_td.iterrows():\n",
    "        close = row['close']\n",
    "        #print(f\"Close: {close}, Max: {max_price}, Min: {min_price}, Counter: {time_counter-idx}\")\n",
    "        if close > max_price:\n",
    "            max_price = close\n",
    "            min_price = max_price - (max_price * pct)\n",
    "        elif close < min_price:\n",
    "            profit = (close - basis) / basis\n",
    "            return [i, profit]\n",
    "        \n",
    "        time_counter += 1\n",
    "        if time_counter > time_stop:\n",
    "            break\n",
    "\n",
    "    # If the loop ends without triggering stop loss, calculate the profit based on the last close\n",
    "    profit = (close - basis) / basis\n",
    "    return [i, profit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_overall = time.time()\n",
    "\n",
    "# Getting all of the trading days because trades can only occur during trading day\n",
    "df['Time'] = df['Datetime'].dt.time\n",
    "start_time = pd.to_datetime('09:00:00').time()\n",
    "end_time = pd.to_datetime('16:00:00').time()\n",
    "TradingDay_df = df[(df['Time'] > start_time) & (df['Time'] < end_time) & (df.index >= 201)]\n",
    "TradingDay_df['Profit'] = TradingDay_df.apply(trail_stoploss, axis=1, tradingdata=TradingDay_df,whole_percentage=5)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Finished calculating Profit in {t2 - time_overall} seconds\")\n",
    "\n",
    "#applying EMAs SMAs\n",
    "EMAs = [8,10,20,50,75,100]\n",
    "SMAs = [8,10,20,50,75,100]\n",
    "Other_tas = [\"RSI14\",\"Bolinger Bands\"]\n",
    "\n",
    "\n",
    "start_index = max(0,df.index.get_loc(TradingDay_df.index[0]) - 201)\n",
    "end_index = start_index + len(TradingDay_df) + 202  \n",
    "sub_df = df.iloc[start_index:end_index]\n",
    "\n",
    "for EMA in EMAs:\n",
    "    sub_df[f'EMA{EMA}'] = ta.ema(sub_df['close'], length=EMA)\n",
    "    df.loc[sub_df.index, f'EMA{EMA}'] = sub_df[f'EMA{EMA}']\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "print(f\"Finished calculating EMAs in {t3 - t2} seconds\")\n",
    "\n",
    "for SMA in SMAs:\n",
    "    sub_df[f'SMA{SMA}'] = ta.sma(sub_df['close'], length=SMA)\n",
    "    df.loc[sub_df.index, f'SMA{SMA}'] = sub_df[f'SMA{SMA}']\n",
    "\n",
    "t4 = time.time()\n",
    "\n",
    "print(f\"Finished calculating SMAs in {t4 - t2} seconds\")\n",
    "\n",
    "# Calculating 1st 2nd Deritivatives\n",
    "# 1st\n",
    "sub_df['Time_Diff'] = sub_df['Datetime'].diff().dt.total_seconds()\n",
    "sub_df['dydx'] = sub_df['close'].diff() / sub_df['Time_Diff']\n",
    "df.loc[sub_df.index, 'dydx'] = sub_df['dydx']\n",
    "\n",
    "t5 = time.time()\n",
    "\n",
    "print(f\"Finished calculating 1st Derivative in {t5 - t4} seconds\")\n",
    "\n",
    "# 2nd\n",
    "sub_df['dydx2'] = sub_df['dydx'].diff() / sub_df['Time_Diff']\n",
    "df.loc[sub_df.index, 'dydx2'] = sub_df['dydx2']\n",
    "\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"Finished calculating 2nd Derivative in {time_end - t5} seconds\")\n",
    "print(f\"time taken for complete data processing = {time_overall-time_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Testing Variables\n",
    "Data_order = [\"Random\",\"Sorted\"]\n",
    "Underlying_data = [\"Percent Change\",\"Raw Data\"]\n",
    "Field_size = [10,20,50,100,200,300]\n",
    "Derivatives = [\"First\",[\"First\",\"Second\"]]\n",
    "TAs = [[\"None\"],[\"EMA\"],[\"EMA\",\"SMA\"],[\"EMA\",\"Other\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = { # From the Grid Search Previously Done\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'colsample_bytree': 0.6,\n",
    "    'gamma': 0,\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 3,\n",
    "    'n_estimators': 300,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
